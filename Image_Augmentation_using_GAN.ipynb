{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Santhosh130903/TNSDC-GEN_AI/blob/main/Image_Augmentation_using_GAN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8rnx-32_14tC"
      },
      "outputs": [],
      "source": [
        "from keras.datasets import mnist\n",
        "from keras.layers import Input, Dense, Reshape, Flatten\n",
        "from keras.layers import Activation\n",
        "\n",
        "from keras.layers import LeakyReLU\n",
        "from keras.models import Sequential, Model\n",
        "from keras.optimizers import Adam\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import sys\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "img_rows = 28\n",
        "img_cols = 28\n",
        "channels = 1\n",
        "img_shape = (img_rows, img_cols, channels)\n",
        "\n",
        "z_dim = 100\n",
        "\n",
        "def generator(img_shape, z_dim):\n",
        "\n",
        "    model = Sequential()\n",
        "\n",
        "    # Hidden layer\n",
        "    model.add(Dense(128, input_dim=z_dim))\n",
        "\n",
        "    # Leaky ReLU\n",
        "    model.add(LeakyReLU(alpha=0.01))\n",
        "\n",
        "    # Output layer with tanh activation\n",
        "    model.add(Dense(28*28*1, activation='tanh'))\n",
        "    model.add(Reshape(img_shape))\n",
        "\n",
        "    z = Input(shape=(z_dim,))\n",
        "    img = model(z)\n",
        "\n",
        "    return Model(z, img)\n",
        "\n",
        "def discriminator(img_shape):\n",
        "\n",
        "    model = Sequential()\n",
        "\n",
        "    model.add(Flatten(input_shape=img_shape))\n",
        "\n",
        "    # Hidden layer\n",
        "    model.add(Dense(128))\n",
        "\n",
        "    # Leaky ReLU\n",
        "    model.add(LeakyReLU(alpha=0.01))\n",
        "    # Output layer with sigmoid activation\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "    img = Input(shape=img_shape)\n",
        "    prediction = model(img)\n",
        "\n",
        "    return Model(img, prediction)\n",
        "discriminator = discriminator(img_shape)\n",
        "discriminator.compile(loss='binary_crossentropy',\n",
        "                      optimizer=Adam(), metrics=['accuracy'])\n",
        "\n",
        "# Build the Generator\n",
        "generator = generator(img_shape, z_dim)\n",
        "\n",
        "# Generated image to be used as input\n",
        "z = Input(shape=(100,))\n",
        "img = generator(z)\n",
        "\n",
        "# Keep Discriminator’s parameters constant during Generator training\n",
        "discriminator.trainable = False\n",
        "\n",
        "# The Discriminator’s prediction\n",
        "prediction = discriminator(img)\n",
        "\n",
        "# Combined GAN model to train the Generator\n",
        "combined = Model(z, prediction)\n",
        "combined.compile(loss='binary_crossentropy', optimizer=Adam())\n",
        "\n",
        "losses = []\n",
        "accuracies = []\n",
        "\n",
        "def train(iterations, batch_size, sample_interval):\n",
        "\n",
        "    # Load the dataset\n",
        "    #path = \"E:\\images\\PACHA DINO.jpeg\"\n",
        "    #X_train = cv2.imread(path + '\\\\'+ str(i) for i in os.listdir(path))\n",
        "    (X_train, _), (_, _) = mnist.load_data()\n",
        "    data_slice = 3000\n",
        "    X_train = X_train[:data_slice,:]\n",
        "    # Rescale -1 to 1\n",
        "    X_train = X_train / 127.5 - 1.\n",
        "    X_train = np.expand_dims(X_train, axis=3)\n",
        "\n",
        "    # Labels for real and fake examples\n",
        "    real = np.ones((batch_size, 1))\n",
        "    fake = np.zeros((batch_size, 1))\n",
        "\n",
        "    for iteration in range(iterations):\n",
        "\n",
        "        # -------------------------\n",
        "        #  Train the Discriminator\n",
        "        # -------------------------\n",
        "\n",
        "        # Select a random batch of real images\n",
        "        idx = np.random.randint(0, X_train.shape[0], batch_size)\n",
        "        imgs = X_train[idx]\n",
        "\n",
        "        # Generate a batch of fake images\n",
        "        z = np.random.normal(0, 1, (batch_size, 100))\n",
        "        gen_imgs = generator.predict(z)\n",
        "\n",
        "        # Discriminator loss\n",
        "        d_loss_real = discriminator.train_on_batch(imgs, real)\n",
        "        d_loss_fake = discriminator.train_on_batch(gen_imgs, fake)\n",
        "        d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
        "\n",
        "        z = np.random.normal(0, 1, (batch_size, 100))\n",
        "        gen_imgs = generator.predict(z)\n",
        "\n",
        "        # Generator loss\n",
        "        g_loss = combined.train_on_batch(z, real)\n",
        "\n",
        "        if iteration % sample_interval == 0:\n",
        "\n",
        "            # Output training progress\n",
        "            print (\"%d [D loss: %f, acc.: %.2f%%] [G loss: %f]\" %\n",
        "                         (iteration, d_loss[0], 100*d_loss[1], g_loss))\n",
        "\n",
        "            # Save losses and accuracies so they can be plotted after training\n",
        "            losses.append((d_loss[0], g_loss))\n",
        "            accuracies.append(100*d_loss[1])\n",
        "\n",
        "            # Output generated image samples\n",
        "            sample_images(iteration)\n",
        "def sample_images(iteration, image_grid_rows=4, image_grid_columns=4):\n",
        "\n",
        "    # Sample random noise\n",
        "    z = np.random.normal(0, 1,\n",
        "              (image_grid_rows * image_grid_columns, z_dim))\n",
        "\n",
        "    # Generate images from random noise\n",
        "    gen_imgs = generator.predict(z)\n",
        "\n",
        "    # Rescale images to 0-1\n",
        "    gen_imgs = 0.5 * gen_imgs + 0.5\n",
        "\n",
        "\n",
        "    # Set image grid\n",
        "    fig, axs = plt.subplots(image_grid_rows, image_grid_columns,\n",
        "                                    figsize=(4,4), sharey=True, sharex=True)\n",
        "\n",
        "    cnt = 0\n",
        "    for i in range(image_grid_rows):\n",
        "        for j in range(image_grid_columns):\n",
        "            # Output image grid\n",
        "            axs[i,j].imshow(gen_imgs[cnt, :,:,0], cmap='gray')\n",
        "            axs[i,j].axis('off')\n",
        "            cnt += 1\n",
        "import warnings; warnings.simplefilter('ignore')\n",
        "\n",
        "iterations = 20000\n",
        "batch_size = 128\n",
        "sample_interval = 1000\n",
        "\n",
        "# Train the GAN for the specified number of iterations\n",
        "train(iterations, batch_size, sample_interval)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPlyIVb+W2f6EUU6lIzOVYz",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}